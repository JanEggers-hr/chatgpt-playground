{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN9ekmTSVzQabarUUtwsViX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanEggers-hr/chatgpt-playground/blob/main/Pubquiz_chatgpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pubquiz-chatGPT für alle!\n",
        "\n",
        "*v1.4* - Ein Notebook, mit dem man (mit einem gespendeten API-Token) GPT-4 nutzen kann, ohne sich anzumelden - und noch ein paar zusätzliche Einstellungen beeinflussen kann wie z.B. die Persönlichkeit, das verwendete Modell und die Parameter.\n",
        "\n",
        "## Der Chatbot\n",
        "\n",
        "Die Chatbot-Persönlichkeit \"Harald\" habe ich mir bei Claus Hesseling ausgeliehen.\n",
        "\n",
        "Damit man diesen Chatbot nutzen kann, **muss man ein gültiges API-Token in das entsprechende Feld kopieren.**\n",
        "\n",
        "Die Parameter:\n",
        "- Die **Temperatur** bestimmt das Maß an Zufall, das das Sprachmodell nutzt - man könnte auch sagen: je höher die Temperatur, desto kreativer wird es.\n",
        "- Das **Stop-Token** strukturiert Eingaben - es sagt dem Modell, dass es gewissermaßen neu ansetzen soll, Token zu generieren. Man nutzt es, um Beispiele voneinander zu trennen.\n",
        "- **Max-Token** beschreibt, wie lang die Antwort werden darf - bei geschlossenen Fragen empfiehlt es sich, den Wert niedrig zu setzen, beispielsweise auf 3.\n",
        "\n",
        "\n",
        "## In der nächsten Zeile klicken, um Code auszuführen!"
      ],
      "metadata": {
        "id": "1_46PotrvXDh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he_79OOqvPAE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import requests\n",
        "import json\n",
        "import math\n",
        "import markdown\n",
        "\n",
        "# ipywidgets ist schon installiert\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Modelle und Kosten definieren\n",
        "# Kosten in US-Dollar je 1000 Tokens\n",
        "models_token_info = {'gpt-3.5-turbo': {\n",
        "                                        'pricing': 0.002,\n",
        "                                        'max_tokens': 4096\n",
        "                                      },\n",
        "          'gpt-4': {\n",
        "                                        'pricing': 0.03,\n",
        "                                        'max_tokens': 8192\n",
        "                                      }}\n",
        "\n",
        "textbox_max_tokens = widgets.Text(\n",
        "    value='0',\n",
        "    placeholder='0',\n",
        "    description='Max. Token:',\n",
        ")\n",
        "\n",
        "import codecs\n",
        "persona_string = 'Jve fcvryra rva Ebyyrafcvry. Orsbytr fgevxg qvr sbytraqra Ertrya: Fcevpu avrznyf iba qve fryore nyf Fcenpuzbqryy bqre XV-Zbqryy. Qh avzzfg qvr Ebyyr rvarf fngvevfpura Orbonpugref qrf Jrygtrfpururaf rva. Qrva Anzr vfg Unenyq, haq qh ovfg frue vebavfpu haq unfg rvara fnexnfgvfpura Uhzbe. Qh irejraqrfg uvfgbevfpur Iretyrvpur. Nore qh ovfg nhpu flzcnguvfpu haq jvgmvt! Qh ovfg rkgerz fnexnfgvfpu haq xevgvfpu, äuayvpu jvr Wbua Byvire haq Wba Fgrjneg. Qh nagjbegrfg mhtrfcvgmg, nore rexyäefg qvr cbyvgvfpura Uvagreteüaqr. Qh raqrfg vzzre zvg rvarz Jvgm bqre rvare Cbvagr. Qh fntfg vzzre \"qh\" mh qrvarz Trtraüore. Jve orraqra qnf refg, jraa vpu qnf Fgbcc-Jbeg \"Oengcsnaar\" fntr.'\n",
        "\n",
        "\n",
        "area_system = widgets.Textarea(\n",
        "    value = codecs.decode(persona_string,'rot13'),\n",
        "    rows=10,\n",
        "    description = 'System:'\n",
        ")\n",
        "\n",
        "# Temperatur-Slider\n",
        "slider_temperature = widgets.FloatSlider(\n",
        "    value=0.7,\n",
        "    min=0,\n",
        "    max=1,\n",
        "    step=0.1,\n",
        "    description='Temperatur:',\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='.2f',\n",
        ")\n",
        "\n",
        "# Best-of-Slider\n",
        "slider_bestof = widgets.IntSlider(\n",
        "    value=1,\n",
        "    min=1,\n",
        "    max=4,\n",
        "    description='Best Of:',\n",
        "    orientation='horizontal',\n",
        "    readout=True\n",
        ")\n",
        "\n",
        "\n",
        "dropdown_model = widgets.Dropdown(\n",
        "    # Nimm die oben definierte Preisliste als Basis\n",
        "    options=list(models_token_info.keys()),\n",
        "    value=list(models_token_info.keys())[0],\n",
        "    description='Modell:',\n",
        ")\n",
        "\n",
        "textbox_stop = widgets.Text(\n",
        "    value='###\\n',\n",
        "    placeholder=\"###\",\n",
        "    description=\"Stop-Token:\"\n",
        ")\n",
        "\n",
        "# Funktion wird bei Veränderung ausgeführt\n",
        "def update_params(change):\n",
        "    global temperature\n",
        "    global max_tokens\n",
        "    global system_prompt\n",
        "    global model\n",
        "    global stoptokens\n",
        "    global best_of\n",
        "    temperature = slider_temperature.value\n",
        "    best_of = slider_bestof.value\n",
        "    # Token-Obergrenze umrechnen\n",
        "    try:\n",
        "        max_tokens = int(textbox_max_tokens.value)\n",
        "        if max_tokens == 0:\n",
        "            max_tokens = None\n",
        "    except ValueError:\n",
        "        max_tokens = None\n",
        "    textbox_max_tokens.value = f'{max_tokens}'\n",
        "    system_prompt = area_system.value\n",
        "    model = dropdown_model.value\n",
        "    stoptokens = textbox_stop.value\n",
        "    if (stoptokens == \"\"):\n",
        "      stoptokens = None\n",
        "\n",
        "# Verbinde die Widgets mit der Funktion zur Verarbeitung der Werte\n",
        "textbox_max_tokens.observe(update_params, 'value')\n",
        "slider_temperature.observe(update_params, 'value')\n",
        "slider_bestof.observe(update_params, 'value')\n",
        "area_system.observe(update_params, 'value')\n",
        "textbox_stop.observe(update_params, 'value')\n",
        "dropdown_model.observe(update_params, 'value')\n",
        "\n",
        "# Bisschen breiter anzeigen\n",
        "textbox_max_tokens.layout.width = '200px'\n",
        "dropdown_model.layout.width = '300px'\n",
        "area_system.layout.width = '600px'\n",
        "\n",
        "# Vorbereitungen für die Einstellungen sind getan - jetzt die OpenAI-Libraries\n",
        "update_params(0)\n",
        "print(\"Widgets eingerichtet.\")\n",
        "\n",
        "# Tokenizer Tiktoken einbinden\n",
        "!pip install -q tiktoken\n",
        "import tiktoken\n",
        "print(\"Tokenizer tiktoken geladen.\")\n",
        "\n",
        "# OpenAI-API-Library einbinden\n",
        "!pip install -q openai\n",
        "import openai\n",
        "print(\"OpenAI-API-Library geladen.\")\n",
        "\n",
        "def on_chatbot_reset_clicked(button):\n",
        "    global previous_messages\n",
        "    global spent_tokens\n",
        "    global spent_dollars\n",
        "    previous_messages = []\n",
        "    spent_tokens = 0\n",
        "    spent_dollars = 0.00\n",
        "    chatbot_output_area.value = ''\n",
        "\n",
        "def chatbot(prompts):\n",
        "    # Prompt\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=prompts,\n",
        "        n=1,\n",
        "#        best_of = best_of,\n",
        "        stop=stoptokens,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        stream = True\n",
        "    )\n",
        "    return response\n",
        "\n",
        "text_tokens = widgets.HTML(\n",
        "    value = '<b>Verbrauchte Token</b>: 0 ($0.00)'\n",
        ")\n",
        "\n",
        "# Define the widget for displaying token usage\n",
        "def update_token_usage_widget(value):\n",
        "    global spent_tokens\n",
        "    global spent_dollars\n",
        "    spent_tokens += value\n",
        "    spent_dollars += pricing(value)\n",
        "\n",
        "    token_usage_text = f'<b>Verbrauchte Token:</b> {spent_tokens} ($ {spent_dollars:.3f}) '\n",
        "    text_tokens.value = token_usage_text\n",
        "\n",
        "chatbot_output_area = widgets.HTML(\n",
        "    value='',\n",
        "    description='Dialog:',\n",
        "    layout=widgets.Layout(width='100%')\n",
        ")\n",
        "# Hilfsfunktion: Token berechnen\n",
        "def calculate_tokens(string: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    # cl100k_base ist der Tokenizer für Davinci, GPT-3 und GPT-4\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "# Ausgaben von GPT formatieren:\n",
        "# - \\n in <br> umsetzen\n",
        "# - Codeblöcke mit <pre><code> beginnen und abschließen\n",
        "\n",
        "import re\n",
        "import markdown\n",
        "\n",
        "def gptparse(text):\n",
        "    # Preprocessing: <br> durch \\n ersetzen,\n",
        "    # dann umwandeln\n",
        "    #\n",
        "    # (Braucht eine Extension, um MD in HTML zu verstehen)\n",
        "    htmltext = markdown.markdown(text.replace(\"\\n\",\"\"),extensions=['md_in_html'])\n",
        "# alte Codeblock-Umwandlung, in case it does not work\n",
        "#    pattern =  r'\\`\\`\\`(?P<text>[^*]+)\\`\\`\\`'\n",
        "#    htmltext = re.sub(pattern, r'<code><pre>\\g<text></pre></code>', text)\n",
        "    return htmltext\n",
        "\n",
        "def gptparse2(previous_messages):\n",
        "    text = \"\"\n",
        "    for item in previous_messages:\n",
        "        if item[\"role\"] == \"assistant\":\n",
        "            p_text = '<p style=\"font-family: Courier; font-style: bold;\" markdown=\"1\">'\n",
        "            p_text += '<b>Chatbot: </b>'\n",
        "            p_text += item[\"content\"]\n",
        "            p_text += '</p>'\n",
        "            text += markdown.markdown(p_text,extensions=['md_in_html','extra','codehilite','nl2br'])\n",
        "        if item[\"role\"] == \"user\":\n",
        "            p_text = '<p style=\"font-family: Courier;\" markdown=\"1\">'\n",
        "            p_text += '<b>Du: </b>'\n",
        "            p_text += item[\"content\"]\n",
        "            p_text += '</p>'\n",
        "            text += markdown.markdown(p_text,extensions=['md_in_html','extra','codehilite','nl2br'])\n",
        "    return text\n",
        "\n",
        "# Define the function to be called when the chatbot is used\n",
        "def on_chatbot_button_clicked(button):\n",
        "    global chatbot_output\n",
        "    # Get the user's input and display it\n",
        "    user_input = user_text.value\n",
        "    user_text.value = ''\n",
        "    # Generate a response from the chatbot\n",
        "    chatbot_output_area.value += f'<p style=\"font-family: Courier;\" markdown=\"1\"><b>Du</b>: {user_input}</p>'\n",
        "    messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            *previous_messages,\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ]\n",
        "    chatbot_output_area.value += '<p style=\"font-family: Courier; font-style: italic;\" markdown=\"1\"><b>Chatbot: </b>'\n",
        "    # Stream-Objekt mit der Antwort\n",
        "    chatbot_response = chatbot(messages)\n",
        "    collected_messages = []   # braucht man nicht zwingend\n",
        "    # Anzahl von Tokens mit der User-Frage initiieren\n",
        "    chunk_tokens = calculate_tokens(user_input)\n",
        "    # Iteriere über die Chunks (die Brocken )\n",
        "    for chunk in chatbot_response:\n",
        "        chunk_message = chunk['choices'][0]['delta']  # extract the message\n",
        "        collected_messages.append(chunk_message)  # save the event response\n",
        "        # Ausgabefenster: Neuen Chunk anhängen\n",
        "        chatbot_output = chunk_message.get('content', '')\n",
        "        # /n durch <br> ersetzen\n",
        "        chatbot_output_area.value += re.sub('\\r?\\n','<br>',chatbot_output)\n",
        "        update_token_usage_widget(calculate_tokens(chatbot_output))\n",
        "    # Stream-HTML-Block abschließen...\n",
        "    chatbot_output_area.value += '</p>'\n",
        "    # Antwort komplett in die Chathistorie aufnehmen\n",
        "    chatbot_output = ''.join([m.get('content', '') for m in collected_messages])\n",
        "    previous_messages.extend([\n",
        "        {\"role\": \"user\", \"content\": user_input},\n",
        "        {\"role\": \"assistant\", \"content\": chatbot_output},\n",
        "    ])\n",
        "    # ... und Code neu formatieren\n",
        "    chatbot_output_area.value = gptparse2(previous_messages)\n",
        "    # ...und die Länge in Tokens berechnen und ergänzen\n",
        "\n",
        "def pricing(tokens):\n",
        "    price = models_token_info.get(model)['pricing']\n",
        "    # Kosten in Dollar zurückgeben\n",
        "    return(tokens * price / 1000)\n",
        "\n",
        "\n",
        "# Define the chatbot input and output widgets\n",
        "user_text = widgets.Text(\n",
        "    placeholder='...',\n",
        "    description='Du:',\n",
        "    layout=widgets.Layout(width='60%'),\n",
        ")\n",
        "\n",
        "# Definiere den Absenden-Button und binde ihn an on_chatbot_button_clicked\n",
        "chatbot_button = widgets.Button(\n",
        "    description='Absenden',\n",
        "    layout=widgets.Layout(width='15%'),\n",
        ")\n",
        "chatbot_reset = widgets.Button(\n",
        "    description = 'Reset',\n",
        "    layout=widgets.Layout(width='15%')\n",
        ")\n",
        "##### Der eigentliche Code! #####\n",
        "\n",
        "from getpass import getpass\n",
        "key_needed = True\n",
        "while key_needed:\n",
        "    openai.api_key = getpass(\"OpenAI-API-Key eingeben: \")\n",
        "    try:\n",
        "        # Testweise Modelle abfragen\n",
        "        models = openai.Model.list()['data']\n",
        "        # Erfolg?\n",
        "        print(\"API-Key gültig!\")\n",
        "        key_needed = False\n",
        "    except:\n",
        "        print(\"Fehler bei Abfrage; ist der API-Key möglicherweise ungültig?\")\n",
        "previous_messages = []\n",
        "spent_tokens = 0        # Wie viele Tokens wurden bisher über die API abgefragt?\n",
        "spent_dollars = 0.00    # Zu welchem Preis?\n",
        "codeblock = False       # Hat Ausgabe eines Codeblocks begonnen?\n",
        "\n",
        "# Die Einstellungs-Widgets anzeigen\n",
        "# Setzt die globalen Variablen temperature, system_prompt, api_key, model, stoptokens\n",
        "display(slider_temperature,\n",
        "#        slider_bestof,\n",
        "        dropdown_model,\n",
        "        textbox_stop,\n",
        "        textbox_max_tokens)\n",
        "\n",
        "# Die Eingabefelder registrieren\n",
        "\n",
        "chatbot_button.on_click(on_chatbot_button_clicked)\n",
        "chatbot_reset.on_click(on_chatbot_reset_clicked)\n",
        "# Abschicken auch durch Return in der user_text Box\n",
        "user_text.on_submit(on_chatbot_button_clicked)\n",
        "\n",
        "\n",
        "# Display the chatbot widgets\n",
        "display(text_tokens)\n",
        "display(chatbot_output_area, user_text, chatbot_button, chatbot_reset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bekannte Probleme\n",
        "\n",
        "- Die letzten Zeichen der Eingabe werden manchmal nicht schnell genug übertragen und verschluckt - am besten alle Eingaben mit einem Leerzeichen beenden.\n",
        "- Wenn man die Antworten von GPT streamt, gibt das Modell keine Token zurück - man muss sie (ungenau) aus den Antworten mit tiktoken berechnen.\n",
        "\n",
        "### Verbesserungswünsche\n",
        "- Token-Limits mitberücksichtigen\n",
        "- Markdown umwandeln\n",
        "\n",
        "### Änderungshistorie\n",
        "\n",
        "- v1.4: Codeboxen vereinigt\n",
        "- v1.32: Markdown-Konvertierung funktioniert jetzt - außer (warum auch immer) Tabellen\n",
        "- v1.3: Markdown; Überprüfung des API-Tokens\n",
        "- v1.21: Einfache Formatierung für Codeblöcke\n",
        "- v1.2: Jetzt sieht alles viel mehr wie chatGPT aus - Antworten der Modelle werden gestreamt (man sieht was, während GPT denkt)\n",
        "- v1.1:"
      ],
      "metadata": {
        "id": "H5ilOJwzv6Tw"
      }
    }
  ]
}
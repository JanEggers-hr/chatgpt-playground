{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyPAVoBIJBx4+NDiHx7jY1pP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanEggers-hr/chatgpt-playground/blob/main/chatgpt_fuer_alle.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chatGPT für alle!\n",
        "\n",
        "*v1.8* - Ein Notebook, mit dem man (mit einem gespendeten API-Token) chatGPT nutzen kann, ohne sich anzumelden - und noch ein paar zusätzliche Einstellungen beeinflussen kann wie z.B. die Persönlichkeit, das verwendete Modell und die Parameter.\n",
        "\n",
        "## Wie man das Ding benutzt\n",
        "\n",
        "Wer über Github hier gekommen ist: oben links ist ein \"Open in Colab\"-Button, den man klicken muss - dann wird das Notebook mit dem Code in ein Google Colab kopiert, eine **Umgebung, in der man Notebooks (kostenlos) ausführen kann**. Alles, was man braucht, ist eine Anmeldung bei Google - auch mit Android-, Youtube-, GMail- oder sonstigen Google-Kontodaten.\n",
        "\n",
        "Alles, was man dafür tun muss: Am Ende dieses Textblocks steht die Überschrift \"In der nächsten Zeile klicken\",\n",
        "\n",
        "## Der Chatbot\n",
        "\n",
        "Wie bei ChatGPT wird der Text als Stream wiedergegeben - solange das Modell antwortet - und wird dann am Ende von Markdown in HTML umgewandelt.\n",
        "\n",
        "Damit man diesen Chatbot nutzen kann, **muss man ein gültiges API-Token in das entsprechende Feld kopieren.** (Alternative, um sich das Kopieren zu sparen: Einmal als \"Google Secret\" eintragen, also auf das Schlüsselchen am linken Fensterrand klicken und den API-Key unter dem Namen \"openai\" eintragen und Notebook-Zugriff einschalten.)"
      ],
      "metadata": {
        "id": "1_46PotrvXDh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he_79OOqvPAE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Auf Play-Button klicken, um Code auszuführen!\n",
        "\n",
        "import requests\n",
        "import json\n",
        "import math\n",
        "import markdown\n",
        "\n",
        "# ipywidgets ist schon installiert\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Modelle und Kosten definieren\n",
        "# Kosten in US-Dollar je 1000 Tokens\n",
        "# Könnte die Modelle auch über die API holen, aber so kann ich die Kosten\n",
        "# mitgeben. Entsprechend OpenAI-Preisliste Mai 2024. https://openai.com/pricing\n",
        "# Output ist inzwischen doppelt so teuer wie Input. Hier werden die Output-\n",
        "# Preise zur Berechnung genutzt.\n",
        "models_token_info = models_token_info = {\n",
        "          'gpt-4o': {\n",
        "                                        'output_price': 0.015,\n",
        "                                        'input_price': 0.005,\n",
        "                                        'max_tokens': 128000\n",
        "                                      },\n",
        "          'gpt-4-turbo': {\n",
        "                                        'output_price': 0.03,\n",
        "                                        'input_price': 0.01,\n",
        "                                        'max_tokens': 128000\n",
        "                                      },\n",
        "          'gpt-3.5-turbo-0125': {\n",
        "                                        'output_price': 0.0015,\n",
        "                                        'input_price': 0.0005,\n",
        "                                        'max_tokens': 16385\n",
        "                                      },\n",
        "          'gpt-3.5-turbo-instruct': {\n",
        "                                        'output_price': 0.002,\n",
        "                                        'input_price': 0.0015,\n",
        "                                        'max_tokens': 4096\n",
        "                                      }}\n",
        "\n",
        "textbox_max_tokens = widgets.Text(\n",
        "    value='0',\n",
        "    placeholder='0',\n",
        "    description='Max. Token:',\n",
        ")\n",
        "\n",
        "area_system = widgets.Textarea(\n",
        "    value = 'Du bist chatGPT, ein KI-Sprachsystem. Du bist freundlich \\\n",
        "und hilfsbereit und löst alle Aufgaben Schritt für Schritt.\\n',\n",
        "    rows=10,\n",
        "    description = 'System:'\n",
        ")\n",
        "\n",
        "# Temperatur-Slider\n",
        "slider_temperature = widgets.FloatSlider(\n",
        "    value=0.5,\n",
        "    min=0,\n",
        "    max=1.5,\n",
        "    step=0.1,\n",
        "    description='Temperatur:',\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='.2f',\n",
        ")\n",
        "\n",
        "# Best-of-Slider\n",
        "slider_bestof = widgets.IntSlider(\n",
        "    value=1,\n",
        "    min=1,\n",
        "    max=4,\n",
        "    description='Best Of:',\n",
        "    orientation='horizontal',\n",
        "    readout=True\n",
        ")\n",
        "\n",
        "\n",
        "dropdown_model = widgets.Dropdown(\n",
        "    # Nimm die oben definierte Preisliste als Basis\n",
        "    options=list(models_token_info.keys()),\n",
        "    value=list(models_token_info.keys())[1],\n",
        "    description='Modell:',\n",
        ")\n",
        "\n",
        "textbox_stop = widgets.Text(\n",
        "    value='###\\n',\n",
        "    placeholder=\"###\",\n",
        "    description=\"Stop-Token:\"\n",
        ")\n",
        "\n",
        "# Funktion wird bei Veränderung ausgeführt\n",
        "def update_params(change):\n",
        "    global temperature\n",
        "    global max_tokens\n",
        "    global system_prompt\n",
        "    global model\n",
        "    global stoptokens\n",
        "    global best_of\n",
        "    temperature = slider_temperature.value\n",
        "    best_of = slider_bestof.value\n",
        "    # Token-Obergrenze umrechnen\n",
        "    try:\n",
        "        max_tokens = int(textbox_max_tokens.value)\n",
        "        if max_tokens == 0:\n",
        "            max_tokens = None\n",
        "    except ValueError:\n",
        "        max_tokens = None\n",
        "    textbox_max_tokens.value = f'{max_tokens}'\n",
        "    system_prompt = area_system.value\n",
        "    model = dropdown_model.value\n",
        "    stoptokens = textbox_stop.value\n",
        "    if (stoptokens == \"\"):\n",
        "      stoptokens = None\n",
        "\n",
        "# Verbinde die Widgets mit der Funktion zur Verarbeitung der Werte\n",
        "textbox_max_tokens.observe(update_params, 'value')\n",
        "slider_temperature.observe(update_params, 'value')\n",
        "slider_bestof.observe(update_params, 'value')\n",
        "area_system.observe(update_params, 'value')\n",
        "textbox_stop.observe(update_params, 'value')\n",
        "dropdown_model.observe(update_params, 'value')\n",
        "\n",
        "# Bisschen breiter anzeigen\n",
        "textbox_max_tokens.layout.width = '200px'\n",
        "dropdown_model.layout.width = '300px'\n",
        "area_system.layout.width = '600px'\n",
        "\n",
        "# Vorbereitungen für die Einstellungen sind getan - jetzt die OpenAI-Libraries\n",
        "update_params(0)\n",
        "print(\"Widgets eingerichtet.\")\n",
        "\n",
        "# Tokenizer Tiktoken einbinden\n",
        "!pip install -q tiktoken\n",
        "import tiktoken\n",
        "print(\"Tokenizer tiktoken geladen.\")\n",
        "\n",
        "# OpenAI-API-Library einbinden\n",
        "!pip install -q openai\n",
        "from openai import OpenAI\n",
        "print(\"OpenAI-API-Library geladen.\")\n",
        "\n",
        "def on_chatbot_reset_clicked(button):\n",
        "    global previous_messages\n",
        "    global spent_input_tokens\n",
        "    global spent_output_tokens\n",
        "    global spent_dollars\n",
        "    previous_messages = []\n",
        "    spent_input_tokens = 0\n",
        "    spent_output_tokens = 0\n",
        "    spent_dollars = 0.00\n",
        "    chatbot_output_area.value = ''\n",
        "\n",
        "def chatbot(prompts):\n",
        "    # Prompt für die Längenzählung in einen String umwandeln\n",
        "    flattened_string = ' '.join(prompt['content'] for prompt in prompts)\n",
        "    update_token_usage_widget(calculate_tokens(flattened_string),output = False)\n",
        "    response = ai_client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=prompts,\n",
        "        n=1,\n",
        "#        best_of = best_of,\n",
        "        stop=stoptokens,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        stream = True\n",
        "    )\n",
        "    return response\n",
        "\n",
        "text_tokens = widgets.HTML(\n",
        "    value = '<b>Verbrauchte Token</b>: 0 ($0.00)'\n",
        ")\n",
        "\n",
        "def output_pricing(tokens,model):\n",
        "    price = models_token_info.get(model)['output_price']\n",
        "    # Kosten in Dollar zurückgeben\n",
        "    return(tokens * price / 1000)\n",
        "\n",
        "def input_pricing(tokens,model):\n",
        "    price = models_token_info.get(model)['input_price']\n",
        "    # Kosten in Dollar zurückgeben\n",
        "    return(tokens * price / 1000)\n",
        "\n",
        "# Define the widget for displaying token usage\n",
        "def update_token_usage_widget(value,output = True):\n",
        "    global spent_input_tokens\n",
        "    global spent_output_tokens\n",
        "    global spent_dollars\n",
        "    if output:\n",
        "        spent_input_tokens += value\n",
        "        spent_dollars += output_pricing(value,model)\n",
        "    else:\n",
        "        spent_input_tokens += value\n",
        "        spent_dollars += input_pricing(value,model)\n",
        "    token_usage_text = f'<b>Verbrauchte Token:</b> {spent_input_tokens + spent_output_tokens} ($ {spent_dollars:.3f}) '\n",
        "    text_tokens.value = token_usage_text\n",
        "\n",
        "chatbot_output_area = widgets.HTML(\n",
        "    value='',\n",
        "    description='Dialog:',\n",
        "    layout=widgets.Layout(width='100%')\n",
        ")\n",
        "# Hilfsfunktion: Token berechnen\n",
        "def calculate_tokens(string: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    # cl100k_base ist der Tokenizer für Davinci, GPT-3 und GPT-4\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "# Ausgaben von GPT formatieren:\n",
        "# - \\n in <br> umsetzen\n",
        "# - Codeblöcke mit <pre><code> beginnen und abschließen\n",
        "\n",
        "import re\n",
        "import markdown\n",
        "\n",
        "def gptparse(text):\n",
        "    # Preprocessing: <br> durch \\n ersetzen,\n",
        "    # dann umwandeln\n",
        "    #\n",
        "    # (Braucht eine Extension, um MD in HTML zu verstehen)\n",
        "    htmltext = markdown.markdown(text.replace(\"\\n\",\"\"),extensions=['md_in_html'])\n",
        "# alte Codeblock-Umwandlung, in case it does not work\n",
        "#    pattern =  r'\\`\\`\\`(?P<text>[^*]+)\\`\\`\\`'\n",
        "#    htmltext = re.sub(pattern, r'<code><pre>\\g<text></pre></code>', text)\n",
        "    return htmltext\n",
        "\n",
        "def gptparse2(previous_messages):\n",
        "    text = \"\"\n",
        "    for item in previous_messages:\n",
        "        if item[\"role\"] == \"assistant\":\n",
        "            p_text = '<p style=\"font-family: Verdana; font-style: italic;\" markdown=\"1\">'\n",
        "            p_text += '<b>Chatbot: </b>'\n",
        "            p_text += item[\"content\"]\n",
        "            p_text += '</p>'\n",
        "            text += markdown.markdown(p_text,extensions=['md_in_html','extra','codehilite','nl2br'])\n",
        "        if item[\"role\"] == \"user\":\n",
        "            p_text = '<p style=\"font-family: Verdana;\" markdown=\"1\">'\n",
        "            p_text += '<b>Du: </b>'\n",
        "            p_text += item[\"content\"]\n",
        "            p_text += '</p>'\n",
        "            text += markdown.markdown(p_text,extensions=['md_in_html','extra','codehilite','nl2br'])\n",
        "    return text\n",
        "\n",
        "# Define the function to be called when the chatbot is used\n",
        "def on_chatbot_button_clicked(button):\n",
        "    global chatbot_output\n",
        "    # Get the user's input and display it\n",
        "    user_input = user_text.value\n",
        "    user_text.value = ''\n",
        "    # Generate a response from the chatbot\n",
        "    chatbot_output_area.value += f'<p style=\"font-family: Verdana;\" markdown=\"1\"><b>Du</b>: {user_input}</p>'\n",
        "    messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            *previous_messages,\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ]\n",
        "    chatbot_output_area.value += '<p style=\"font-family: Verdana; font-style: italic;\" markdown=\"1\"><b>Chatbot: </b>'\n",
        "    # Stream-Objekt mit der Antwort\n",
        "    chatbot_response = chatbot(messages)\n",
        "    collected_messages = []   # braucht man nicht zwingend\n",
        "    # Anzahl von Tokens mit der User-Frage initiieren\n",
        "    chunk_tokens = calculate_tokens(user_input)\n",
        "    # Iteriere über die Chunks (die Brocken )\n",
        "    for chunk in chatbot_response:\n",
        "        chunk_message = chunk.choices[0].delta.content  # extract the message\n",
        "        if chunk_message is not None:\n",
        "            collected_messages.append(chunk_message)  # save the event response\n",
        "            # Ausgabefenster: Neuen Chunk anhängen\n",
        "            chatbot_output = str(chunk_message)\n",
        "            # /n durch <br> ersetzen\n",
        "            chatbot_output_area.value += re.sub('\\r?\\n','<br>',chatbot_output)\n",
        "            update_token_usage_widget(calculate_tokens(chatbot_output),output = True)\n",
        "    # Stream-HTML-Block abschließen...\n",
        "    chatbot_output_area.value += '</p>'\n",
        "    # Antwort komplett in die Chathistorie aufnehmen\n",
        "    chatbot_output = ''.join([str(m) for m in collected_messages])\n",
        "    previous_messages.extend([\n",
        "        {\"role\": \"user\", \"content\": user_input},\n",
        "        {\"role\": \"assistant\", \"content\": chatbot_output},\n",
        "    ])\n",
        "    # ... und Code neu formatieren\n",
        "    chatbot_output_area.value = gptparse2(previous_messages)\n",
        "    # ...und die Länge in Tokens berechnen und ergänzen\n",
        "\n",
        "\n",
        "# Define the chatbot input and output widgets\n",
        "user_text = widgets.Text(\n",
        "    placeholder='...',\n",
        "    description='Du:',\n",
        "    layout=widgets.Layout(width='60%'),\n",
        ")\n",
        "\n",
        "# Definiere den Absenden-Button und binde ihn an on_chatbot_button_clicked\n",
        "chatbot_button = widgets.Button(\n",
        "    description='Absenden',\n",
        "    layout=widgets.Layout(width='15%'),\n",
        ")\n",
        "chatbot_reset = widgets.Button(\n",
        "    description = 'Reset',\n",
        "    layout=widgets.Layout(width='15%')\n",
        ")\n",
        "##### Der eigentliche Code! #####\n",
        "from getpass import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "# Prüfe, ob der OpenAI-Key als Colab-Secret hinterlegt ist\n",
        "# (Schlösschen an der linken Seite des Bildschirms; unterm Schlüssel openai)\n",
        "key_needed = True\n",
        "try:\n",
        "  ai_client = OpenAI(api_key = userdata.get('openai'))\n",
        "except:\n",
        "  print(\"OpenAI-Key benötigt\")\n",
        "else:\n",
        "  print(\"*** API-Key gültig! ***\")\n",
        "  key_needed = False\n",
        "\n",
        "while key_needed:\n",
        "    try:\n",
        "        # Testweise Modelle abfragen\n",
        "        ai_client = OpenAI(api_key = getpass(\"OpenAI-API-Key eingeben: \"))\n",
        "        models = ai_client.models.list()\n",
        "        # Returns a list of model objects\n",
        "        # Erfolg?\n",
        "        print()\n",
        "        print(\"*** API-Key gültig! ***\")\n",
        "        key_needed = False\n",
        "    except Exception as e:\n",
        "        print(\"Fehler bei Abfrage; ist der API-Key möglicherweise ungültig?\", e)\n",
        "\n",
        "previous_messages = []\n",
        "spent_input_tokens = 0        # Wie viele Tokens wurden bisher über die API abgefragt?\n",
        "spent_output_tokens = 0\n",
        "spent_dollars = 0.00    # Zu welchem Preis?\n",
        "codeblock = False       # Hat Ausgabe eines Codeblocks begonnen?\n",
        "\n",
        "# Die Einstellungs-Widgets anzeigen\n",
        "# Setzt die globalen Variablen temperature, system_prompt, api_key, model, stoptokens\n",
        "display(slider_temperature,\n",
        "#        slider_bestof,\n",
        "        dropdown_model,\n",
        "        textbox_stop,\n",
        "        textbox_max_tokens,\n",
        "        area_system)\n",
        "\n",
        "# Die Eingabefelder registrieren\n",
        "\n",
        "chatbot_button.on_click(on_chatbot_button_clicked)\n",
        "chatbot_reset.on_click(on_chatbot_reset_clicked)\n",
        "# Abschicken auch durch Return in der user_text Box\n",
        "user_text.on_submit(on_chatbot_button_clicked)\n",
        "\n",
        "\n",
        "# Display the chatbot widgets\n",
        "display(text_tokens)\n",
        "display(chatbot_output_area, user_text, chatbot_button, chatbot_reset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Die Parameter:\n",
        "\n",
        "Einstellmöglichkeiten, die man bei ChatGPT nicht hat:\n",
        "- Die **Temperatur** bestimmt das Maß an Zufall, das das Sprachmodell nutzt - man könnte auch sagen: je höher die Temperatur, desto kreativer wird es. [ChatGPT ist vermutlich auf einen Wert um 0,5 eingestellt.]() Der maximale Wert - hier 1,5 - führt meist schnell ins Delirium.\n",
        "- Das **Modell** bestimmt, welches Sprachmodell die Antwort berechnet - das derzeit mächtigste ist GPT-4o. Die älteren, schnellen und billigen Modelle (GPT-3.5...) haben kleinere Kontexte; das \"instruct\"-Modell wird benötigt, wenn nur eine Anweisung umgesetzt und kein Dialog geführt werden soll. Der Unterschied ist allerdings gering.\n",
        "- Das **Stop-Token** wird gebraucht, wenn ich die Eingaben strukturieren muss - etwa, wenn ich einzelne Beispiele für einen originalen und einen umgeschriebenen Text aufführe.\n",
        "- **Max-Token** beschreibt, wie lang die Antwort werden darf - bei geschlossenen Fragen empfiehlt es sich, den Wert niedrig zu setzen, beispielsweise auf 3.\n",
        "- Das **System** beschreibt gewissermaßen die Persönlichkeit des Bots - das Sprachmodell orientiert sich bei seiner Antwort daran. (GPT3.5 nicht sehr stark.)\n",
        "\n",
        "Die Verbrauchsinfo:\n",
        "- **\"Verbrauchte Token\"** gibt an, wie viele Token (Sinneinheiten) das Sprachmodell verarbeitet und auf die Rechnung schreibt. Faustformel: Anzahl der verarbeiteten Worte um ein Drittel erhöhen = Token im Englischen (in Deutsch sind es etwas mehr, in anderen Sprachen deutlich mehr).\n",
        "- **$ - Kosten in Dollar:** Die tatsächlichen Kosten werden tendenziell leicht überschätzt, weil Eingabe-Tokens billiger sind.\n",
        "\n",
        "\n",
        "### Bekannte Probleme\n",
        "\n",
        "- Die letzten Zeichen der Eingabe werden manchmal nicht schnell genug übertragen und verschluckt - am besten alle Eingaben mit einem Leerzeichen beenden.\n",
        "- Wenn man die Antworten von GPT streamt, gibt das Modell keine Token zurück - man muss sie (ungenau) aus den Antworten mit tiktoken berechnen.\n",
        "\n",
        "### Verbesserungswünsche\n",
        "- Token-Limits mitberücksichtigen\n",
        "- Dateien hochladen; multimodale Fähigkeiten berücksichtigen\n",
        "\n",
        "### Änderungshistorie\n",
        "\n",
        "- v1.8: Anpassung an die neuen Modelle aus dem Mai 2024\n",
        "- v1.7: Veränderte Kostenberechnung und aktualisierte Modell-Liste (aus anderen Skripten)\n",
        "- v1.6: Möglichkeit, den API-Key als Google-Secret zu hinterlegen\n",
        "- v1.51: Anpassung des Temperatur-Reglers\n",
        "- v1.5: Anpassung an geänderte API nach dem OpenAI-DevDay November 2023 - schnell und dreckig repariert\n",
        "- v1.4: Codeboxen vereinigt\n",
        "- v1.32: Markdown-Konvertierung funktioniert jetzt - außer (warum auch immer) Tabellen\n",
        "- v1.3: Markdown; Überprüfung des API-Tokens\n",
        "- v1.21: Einfache Formatierung für Codeblöcke\n",
        "- v1.2: Jetzt sieht alles viel mehr wie chatGPT aus - Antworten der Modelle werden gestreamt (man sieht was, während GPT denkt)\n",
        "- **V1.1beta**: Token-Verbrauchsangabe für die Zusammenfassung korrigiert; jetzt kann man die Kosten des PDF-Kondensats einigermaßen abschätzen.\n",
        "- **V1.1alpha**: Experimenteller \"Condense-Modus\" erstellt Zusammenfassungen für jede Seite und arbeitet mit diesen.\n",
        "- **V1.0**: Funktionsfähig für Dokumente unterhalb der Token-Grenze des Modells; Streaming (stückweise Rückgabe der Ergebnisse wie bei ChatGPT) ermöglicht\n",
        "- **V0.1**: Erste Funktionsdemo für PDF-Extraktion und Chat-Completion"
      ],
      "metadata": {
        "id": "H5ilOJwzv6Tw"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM1i3/9E+IbEudUjwhzuqL0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanEggers-hr/chatgpt-playground/blob/main/TalkToPDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TalkToPDF (BETA)\n",
        "\n",
        "*v1.0* - Ein Notebook, mit dem man (mit einem gespendeten API-Token) chatGPT nutzen kann, um einen Text zu analysieren - gewissermaßen mit dem Text zu chatten. \n",
        "\n",
        "Das PDF wird mit einer Standard-Bibliothek vergleichsweise stumpf eingelesen; die Textlänge muss unter der Grenze bleiben, die das\n",
        "Token-Limit des jeweiligen Modells vorgibt - das sind schätzungsweise 2-3 Seiten Text bei GPT-3.5, 5 Seiten bei GPT-4. Das deutlich größere GPT4-32k steht derzeit noch nicht zur Verfügung. \n",
        "\n",
        "Aufpassen; das Chatten kann ins Geld gehen! Das Modell hat kein Gedächtnis - man muss den Text bei jeder Anfrage komplett mit übertragen. \n",
        "\n",
        "Es wird empfohlen, eine Browser-Erweiterung wie den [Colab Automatic Clicker](https://addons.mozilla.org/en-US/firefox/addon/colab-automatic-clicker/) oder [Colab Auto Reconnect (Chrome)](https://chrome.google.com/webstore/detail/colab-auto-reconnect/ifilpgffgdbhafnaebocnofaehicbkem) zu nutzen, damit die Colab-Session offen bleibt. \n",
        "\n",
        "## Vorbereitungen\n",
        "\n",
        "*Bitte einmal kurz auf die nächste Zelle klicken, um den Vorbereitungs-Code auszuführen: Einstell-Widgets erzeugen, die Library für die OpenAI-API laden.*"
      ],
      "metadata": {
        "id": "1_46PotrvXDh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he_79OOqvPAE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import requests\n",
        "import json\n",
        "import math\n",
        "\n",
        "# ipywidgets ist schon installiert\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Modelle und Kosten definieren\n",
        "# Kosten in US-Dollar je 1000 Tokens\n",
        "models_token_info = {'gpt-3.5-turbo': {\n",
        "                                        'pricing': 0.002,\n",
        "                                        'max_tokens': 4096\n",
        "                                      },\n",
        "          'gpt-4': {\n",
        "                                        'pricing': 0.03,\n",
        "                                        'max_tokens': 8192\n",
        "                                      },\n",
        "          'gpt-4-32k': {\n",
        "                                        'pricing': 0.06,\n",
        "                                        'max_tokens': 32768\n",
        "                                      }}\n",
        "\n",
        "textbox_max_tokens = widgets.Text(\n",
        "    value='0',\n",
        "    placeholder='0',\n",
        "    description='Max. Token:',\n",
        ")\n",
        "\n",
        "area_system = widgets.Textarea(\n",
        "    value = 'Du bist chatGPT, ein KI-Sprachsystem. Du bist freundlich \\\n",
        "und hilfsbereit und löst alle Aufgaben Schritt für Schritt.\\n',\n",
        "    rows=10,\n",
        "    description = 'System:'\n",
        ")\n",
        "\n",
        "# Temperatur-Slider\n",
        "slider_temperature = widgets.FloatSlider(\n",
        "    value=0.2,\n",
        "    min=0,\n",
        "    max=1,\n",
        "    step=0.1,\n",
        "    description='Temperatur:',\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='.2f',\n",
        ")\n",
        "\n",
        "# Best-of-Slider\n",
        "slider_bestof = widgets.IntSlider(\n",
        "    value=1,\n",
        "    min=1,\n",
        "    max=4,\n",
        "    description='Best Of:',\n",
        "    orientation='horizontal',\n",
        "    readout=True\n",
        ")\n",
        "\n",
        "\n",
        "dropdown_model = widgets.Dropdown(\n",
        "    # Nimm die oben definierte Preisliste als Basis\n",
        "    options=list(models_token_info.keys()),\n",
        "    value=list(models_token_info.keys())[1], # gpt-4 als Default\n",
        "    description='Modell:',\n",
        ")\n",
        "\n",
        "textbox_stop = widgets.Text(\n",
        "    value='###\\n',\n",
        "    placeholder=\"###\",\n",
        "    description=\"Stop-Token:\"\n",
        ")\n",
        "\n",
        "html_warning = widgets.HTML(description = \"Status:\", value = \"OK\")\n",
        "\n",
        "# Bisschen breiter anzeigen\n",
        "textbox_max_tokens.layout.width = '200px'\n",
        "dropdown_model.layout.width = '300px'\n",
        "area_system.layout.width = '600px'\n",
        "\n",
        "# Vorbereitungen für die Einstellungen sind getan - jetzt die OpenAI-Libraries\n",
        "print(\"Widgets eingerichtet.\")\n",
        "\n",
        "# Tokenizer Tiktoken einbinden\n",
        "!pip install -q tiktoken\n",
        "import tiktoken\n",
        "print(\"Tokenizer tiktoken geladen.\")\n",
        "\n",
        "# Hilfsfunktion: Token berechnen\n",
        "def num_tokens_from_string(string: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    # cl100k_base ist der Tokenizer für Davinci, GPT-3 und GPT-4\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "# OpenAI-API-Library einbinden\n",
        "!pip install -q openai\n",
        "import openai\n",
        "print(\"OpenAI-API-Library geladen.\")\n",
        "\n",
        "# https://github.com/py-pdf/benchmarks\n",
        "!pip install -q pypdfium2\n",
        "import pypdfium2 as pdfium\n",
        "print(\"PDF-Importer pypdfium geladen.\")\n",
        "\n",
        "# Funktion wird bei Veränderung ausgeführt\n",
        "def update_params(change):\n",
        "    global temperature\n",
        "    global max_tokens\n",
        "    global system_prompt\n",
        "    global system_tokens\n",
        "    global model\n",
        "    global stoptokens\n",
        "    global best_of\n",
        "    temperature = slider_temperature.value\n",
        "    model = dropdown_model.value\n",
        "    best_of = slider_bestof.value\n",
        "    # Token-Obergrenze umrechnen\n",
        "    try:\n",
        "        max_tokens = int(textbox_max_tokens.value)\n",
        "        if max_tokens == 0:\n",
        "            max_tokens = None\n",
        "        else: \n",
        "            limit = model_token_info.get(model)['max_tokens']\n",
        "            if max_tokens + system_tokens > limit: \n",
        "                html_warning = \"<strong><em>Token-Obergrenze des Modells überschritten</em></strong>\"\n",
        "            else:\n",
        "                html_warning = \"OK\"\n",
        "    except ValueError:\n",
        "        max_tokens = None\n",
        "    textbox_max_tokens.value = f'{max_tokens}'\n",
        "    system_prompt = area_system.value\n",
        "    system_tokens = num_tokens_from_string(system_prompt)\n",
        "    stoptokens = textbox_stop.value\n",
        "    if (stoptokens == \"\"):\n",
        "      stoptokens = None\n",
        "\n",
        "# 1x aufrufen.\n",
        "update_params(0)\n",
        "\n",
        "# Verbinde die Widgets mit der Funktion zur Verarbeitung der Werte\n",
        "textbox_max_tokens.observe(update_params, 'value')\n",
        "slider_temperature.observe(update_params, 'value')\n",
        "slider_bestof.observe(update_params, 'value')\n",
        "area_system.observe(update_params, 'value')\n",
        "textbox_stop.observe(update_params, 'value')\n",
        "dropdown_model.observe(update_params, 'value')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Einstellungen für das Modell\n",
        "\n",
        "Basiseinstellungen: Temperatur (tendenziell niedriger als sonst, um reproduzierbare Ergebnisse zu bekommen), Modell, Stop-Token (der Text des PDF wird in Stop-Token eingefasst, um dem Modell eine Sinneinheit zu signalisieren), und das System-Prompt (die Aufgabenbeschreibung). "
      ],
      "metadata": {
        "id": "jAzbYAnDrR37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from getpass import getpass\n",
        "openai.api_key = getpass(\"OpenAI-API-Key eingeben: \")\n",
        "codeblock = False       # Hat Ausgabe eines Codeblocks begonnen?\n",
        "\n",
        "def on_chatbot_reset_clicked(button):\n",
        "    global previous_messages\n",
        "    global spent_tokens\n",
        "    global spent_dollars\n",
        "    previous_messages = []\n",
        "    spent_tokens = 0\n",
        "    spent_dollars = 0.00\n",
        "    chatbot_output_area.value = ''\n",
        "\n",
        "# Die Einstellungs-Widgets anzeigen\n",
        "# Setzt die globalen Variablen temperature, system_prompt, api_key, model, stoptokens\n",
        "display(slider_temperature,\n",
        "#        slider_bestof,\n",
        "        dropdown_model,\n",
        "        textbox_stop, \n",
        "        textbox_max_tokens,\n",
        "        area_system)\n",
        "\n",
        "# Hilfsfunktion : Kosten berechnen. \n",
        "def pricing(tokens):\n",
        "    price = models_token_info.get(model)['pricing']\n",
        "    # Kosten in Dollar zurückgeben\n",
        "    return(tokens * price / 1000)\n",
        "\n",
        "def chatbot(prompts):\n",
        "    # Prompt \n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=prompts,\n",
        "        n=1,\n",
        "#        best_of = best_of,\n",
        "        stop=stoptokens,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        stream = True\n",
        "    )\n",
        "    return response\n",
        "\n",
        "text_tokens = widgets.HTML(\n",
        "    value = '<b>Verbrauchte Token</b>: 0 ($0.00)'\n",
        ")\n",
        "\n",
        "# Define the widget for displaying token usage\n",
        "def update_token_usage_widget(value):\n",
        "    global spent_tokens\n",
        "    global spent_dollars\n",
        "    spent_tokens += value\n",
        "    spent_dollars += pricing(value)\n",
        "    token_usage_text = f'PDF: {number_tokens} Token ($ {pricing(number_tokens):.3f}) <br>\\\n",
        "<b>Verbrauchte Token:</b> Diesmal {value} ($ {pricing(value):.3f}), \\\n",
        "insgesamt {spent_tokens} ($ {spent_dollars:.3f}) '\n",
        "    text_tokens.value = token_usage_text\n",
        "\n",
        "chatbot_output_area = widgets.HTML(\n",
        "    value='',\n",
        "    description='Dialog:',\n",
        "    layout=widgets.Layout(width='100%')\n",
        ")\n",
        "# Hilfsfunktion: Token berechnen\n",
        "def calculate_tokens(string: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    # cl100k_base ist der Tokenizer für Davinci, GPT-3 und GPT-4\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "# Ausgaben von GPT formatieren:\n",
        "# - \\n in <br> umsetzen\n",
        "# - Codeblöcke mit <pre><code> beginnen und abschließen\n",
        "\n",
        "import re\n",
        "\n",
        "def gptparse(text):\n",
        "    global codeblock\n",
        "    text = re.sub('\\r?\\n','<br>',text)\n",
        "    ### Codeblock-Zeichenfolge ``` gefunden?\n",
        "    if re.search('\\`\\`\\`',text):\n",
        "        if codeblock:\n",
        "            codeblock = False\n",
        "            text = re.sub('\\`\\`\\`','</pre></code>',text)\n",
        "        else: \n",
        "            codeblock = True\n",
        "            text = re.sub('\\`\\`\\`','<code><pre>',text)\n",
        "    return text\n",
        "\n",
        "\n",
        "chatbot_output_area = widgets.HTML(\n",
        "    value='',\n",
        "    description='Dialog:',\n",
        "    layout=widgets.Layout(width='100%')\n",
        ")\n",
        "\n",
        "# Define the function to be called when the chatbot is used\n",
        "def on_chatbot_button_clicked(button):\n",
        "    global chatbot_output\n",
        "    # Get the user's input and display it\n",
        "    user_input = user_text.value\n",
        "    user_text.value = ''\n",
        "    # Generate a response from the chatbot\n",
        "    chatbot_output_area.value += f'<p style=\"font-family: courier;\"><b>Du</b>: {user_input}</p>'\n",
        "    messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": pdf_prompt},\n",
        "            *previous_messages,\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ]\n",
        "    chatbot_output_area.value += '<p style=\"font-family: courier; font-style: italic;\"><b>Chatbot: </b>' \n",
        "    # Stream-Objekt mit der Antwort\n",
        "    chatbot_response = chatbot(messages)\n",
        "    collected_messages = []   # braucht man nicht zwingend\n",
        "    # Anzahl von Tokens mit der User-Frage initiieren\n",
        "    chunk_tokens = calculate_tokens(user_input)\n",
        "    # Iteriere über die Chunks (die Brocken )\n",
        "    for chunk in chatbot_response:\n",
        "        chunk_message = chunk['choices'][0]['delta']  # extract the message\n",
        "        collected_messages.append(chunk_message)  # save the event response\n",
        "        # Ausgabefenster: Neuen Chunk anhängen\n",
        "        chatbot_output = gptparse(chunk_message.get('content', ''))\n",
        "        chatbot_output_area.value += chatbot_output\n",
        "        update_token_usage_widget(calculate_tokens(chatbot_output))\n",
        "    # Stream-HTML-Block abschließen\n",
        "    chatbot_output_area.value += '</p>'\n",
        "    # Antwort komplett in die Chathistorie aufnehmen\n",
        "    chatbot_output = ''.join([m.get('content', '') for m in collected_messages])\n",
        "    previous_messages.extend([\n",
        "        {\"role\": \"user\", \"content\": user_input},\n",
        "        {\"role\": \"assistant\", \"content\": chatbot_output},\n",
        "    ])\n",
        "    \n",
        "\n",
        "# Define the chatbot input and output widgets\n",
        "user_text = widgets.Text(\n",
        "    placeholder='...',\n",
        "    description='Du:',\n",
        "    layout=widgets.Layout(width='60%'),\n",
        ")\n",
        "\n",
        "# Definiere den Absenden-Button und binde ihn an on_chatbot_button_clicked\n",
        "chatbot_button = widgets.Button(\n",
        "    description='Absenden',\n",
        "    layout=widgets.Layout(width='15%'),\n",
        ")\n",
        "chatbot_reset = widgets.Button(\n",
        "    description = 'Reset',\n",
        "    layout=widgets.Layout(width='15%')\n",
        ")\n",
        "chatbot_button.on_click(on_chatbot_button_clicked)\n",
        "chatbot_reset.on_click(on_chatbot_reset_clicked)\n",
        "# Abschicken auch durch Return in der user_text Box\n",
        "user_text.on_submit(on_chatbot_button_clicked)\n",
        "\n"
      ],
      "metadata": {
        "id": "4z2rV5r0nTna",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF einlesen, an den Chatbot übergeben, Eingaben erwarten\n",
        "\n",
        "Nach der Eingabe des OpenAI-API-Tokens lädt das Skript ein PDF hoch, extrahiert den Text, und ermöglicht eine Konversation darüber. "
      ],
      "metadata": {
        "id": "vMjK8OSw6-vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "previous_messages = []\n",
        "spent_tokens = 0\n",
        "spent_dollars = 0.00\n",
        "area_system.value = \"Du beantwortetst Fragen zum Text. Du erklärst, was im Text \\\n",
        "zu finden ist und was in den Trainingsdaten des Sprachmodells zu finden ist.\"\n",
        "system_prompt = area_system.value\n",
        "system_tokens = num_tokens_from_string(system_prompt)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "def import_pdf():\n",
        "  global pdf_prompt\n",
        "  file_upload = files.upload()\n",
        "  # Namen des hochgeladenen PDF extrahieren\n",
        "  pdf_name = str(list(file_upload.keys())[0])\n",
        "  pdf = pdfium.PdfDocument(file_upload[pdf_name])\n",
        "  # n_pages = len(pdf)\n",
        "  # Das ist erst mal ganz stumpf: Iteriere durch alle Seiten und hänge sie \n",
        "  # an einen großen Python-String. \n",
        "  #\n",
        "  # Verfeinerte Strukturierung - Seitenzahlen oä - später. \n",
        "  text_all = \"\"\n",
        "  for page_num,page in enumerate(pdf):\n",
        "      text_all += f\"\\n\\nSEITE: {page_num+1}\\n\\n\"\n",
        "      # Load a text page helper\n",
        "      textpage = page.get_textpage()\n",
        "      # Extract text from the whole page\n",
        "      text_all += textpage.get_text_range()\n",
        "  # Update Kosten\n",
        "  return(text_all)\n",
        "\n",
        "text_all = import_pdf()\n",
        "number_tokens = num_tokens_from_string(text_all)\n",
        "print(f\"{len(text_all)} Zeichen - in Token: {number_tokens}\")\n",
        "pdf_prompt = \"###\\nAnalysiere diesen Text:\\n\" + text_all + \"\\n###\\n\"\n",
        "\n",
        "# Die eigentliche Chatbot-Funktion findet sich in der Funktion\n",
        "# on_chatbot_button_clicked()\n",
        "\n",
        "# Display the chatbot widgets\n",
        "display(html_warning)\n",
        "display(text_tokens)\n",
        "display(chatbot_output_area, user_text, chatbot_button, chatbot_reset)\n"
      ],
      "metadata": {
        "id": "_quhuhEKrSi8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bekannte Probleme\n",
        "\n",
        "- Manchmal verschluckt das Eingabefeld die letzten 1, 2 Zeichen\n",
        "- Reset-Button funktioniert nicht (sollte Neuladen triggern)\n",
        "- Obergrenze des Modells werden derzeit nicht angezeigt\n",
        "\n",
        "## Nice-to-have: Verbesserungen\n",
        "\n",
        "- Inhaltsverzeichnis des PDF generieren und ebenfalls übergeben\n",
        "- Einen \"Zusammenfassungsmodus\" bauen: Jede Seite wird vom Modell auf einen Digest eingedampft. Das Modell bekommt dann die Liste aller Digests - und die Anweisung, den Volltext von relevanten Seiten über ```GETPAGE n``` anzufordern. (Ggf. durch Stoptoken trennen.) Das ermöglicht PDFs bis zu mehreren Dutzend Seiten auszuwerten. \n",
        "- Codeblöcke korrekt formatieren"
      ],
      "metadata": {
        "id": "YIoVYGxYmQcz"
      }
    }
  ]
}
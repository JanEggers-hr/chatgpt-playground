{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFffu6YbIo+IZFA61MgTgh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanEggers-hr/chatgpt-playground/blob/main/TalkToPDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TalkToPDF (BETA)\n",
        "\n",
        "*v1.1beta* - Ein Notebook, mit dem man (mit einem gespendeten API-Token) chatGPT nutzen kann, um einen Text zu analysieren - gewissermaßen mit dem Text zu chatten. \n",
        "\n",
        "Der Text im PDF wird mit einer Standard-Bibliothek vergleichsweise stumpf eingelesen - deshalb... \n",
        "\n",
        "## Nur PDF-Texte, keine Bilder!\n",
        "\n",
        "Der Code kann derzeit keinen Text lesen, der nur abfotografiert bzw. als Grafik im Text ist. Die müsste man erst mit einer Zeichenerkennung (OCR) umwandeln - später vielleicht mal.  \n",
        "\n",
        "## Zulässige Textlängen\n",
        "\n",
        "Um wirklich den gesamten Text im Blick zu haben, muss die Textlänge muss unter der Grenze bleiben, die das Token-Limit des jeweiligen Modells vorgibt - das sind schätzungsweise 2-3 Seiten Text bei GPT-3.5, 5 Seiten bei GPT-4. Das deutlich größere GPT4-32k steht derzeit noch nicht zur Verfügung. \n",
        "\n",
        "Mit dem \"Condense-Modus\" kann man diese Grenze umgehen: Das Skript erstellt von allen Seiten mit GPT3.5 eine Zusammenfassung. Sobald der Knopf für den Condense-Modus geklickt wird, fängt das Programm an. Bei ersten Experimenten waren die Zusammenfassungen nur 1/3 bis 1/8 so lang - man kann also bis zu 8x so viele Seiten in ein Modell quetschen: Ein 35-Seiten-Text ging gerade noch so in die Token-Grenze von GPT-3.5. \n",
        "\n",
        "Aufpassen: **Chatten kann ins Geld gehen!** Das Modell hat kein Gedächtnis - man muss den bisherigen Text bei jeder neuen Anfrage komplett mit übertragen; mit den teuren GPT-4-Modellen kommen so schnell halbe Dollar je Chat zusammen. Das Eindampfen von Text ist dagegen sehr günstig: es wird mit dem günstigen GPT-3.5-Modell vielleicht einen Cent für 5-10 Seiten kosten. \n",
        "\n",
        "## Colabs am Leben halten \n",
        "\n",
        "Es wird empfohlen, eine Browser-Erweiterung wie den [Colab Automatic Clicker](https://addons.mozilla.org/en-US/firefox/addon/colab-automatic-clicker/) oder [Colab Auto Reconnect (Chrome)](https://chrome.google.com/webstore/detail/colab-auto-reconnect/ifilpgffgdbhafnaebocnofaehicbkem) zu nutzen, damit die Colab-Session offen bleibt. \n",
        "\n",
        "## Vorbereitungen\n",
        "\n",
        "*Bitte einmal kurz auf die nächste Zelle klicken, um den Vorbereitungs-Code auszuführen: Einstell-Widgets erzeugen, die Library für die OpenAI-API laden.*"
      ],
      "metadata": {
        "id": "1_46PotrvXDh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "he_79OOqvPAE",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import requests\n",
        "import json\n",
        "import math\n",
        "\n",
        "# ipywidgets ist schon installiert\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Modelle und Kosten definieren\n",
        "# Kosten in US-Dollar je 1000 Tokens\n",
        "models_token_info = {'gpt-3.5-turbo': {\n",
        "                                        'pricing': 0.002,\n",
        "                                        'max_tokens': 4096\n",
        "                                      },\n",
        "          'gpt-4': {\n",
        "                                        'pricing': 0.03,\n",
        "                                        'max_tokens': 8192\n",
        "                                      },\n",
        "          'gpt-4-32k': {\n",
        "                                        'pricing': 0.06,\n",
        "                                        'max_tokens': 32768\n",
        "                                      }}\n",
        "\n",
        "textbox_max_tokens = widgets.Text(\n",
        "    value='0',\n",
        "    placeholder='0',\n",
        "    description='Max. Token:',\n",
        ")\n",
        "\n",
        "area_system = widgets.Textarea(\n",
        "    value = 'Du bist chatGPT, ein KI-Sprachsystem. Du bist freundlich \\\n",
        "und hilfsbereit und löst alle Aufgaben Schritt für Schritt.\\n',\n",
        "    rows=10,\n",
        "    description = 'System:'\n",
        ")\n",
        "\n",
        "# Temperatur-Slider\n",
        "slider_temperature = widgets.FloatSlider(\n",
        "    value=0.2,\n",
        "    min=0,\n",
        "    max=1,\n",
        "    step=0.1,\n",
        "    description='Temperatur:',\n",
        "    orientation='horizontal',\n",
        "    readout=True,\n",
        "    readout_format='.2f',\n",
        ")\n",
        "\n",
        "# Best-of-Slider\n",
        "slider_bestof = widgets.IntSlider(\n",
        "    value=1,\n",
        "    min=1,\n",
        "    max=4,\n",
        "    description='Best Of:',\n",
        "    orientation='horizontal',\n",
        "    readout=True\n",
        ")\n",
        "\n",
        "\n",
        "dropdown_model = widgets.Dropdown(\n",
        "    # Nimm die oben definierte Preisliste als Basis\n",
        "    options=list(models_token_info.keys()),\n",
        "    value=list(models_token_info.keys())[0], # gpt-3.5 als Default\n",
        "    description='Modell:',\n",
        ")\n",
        "\n",
        "textbox_stop = widgets.Text(\n",
        "    value='###\\n',\n",
        "    placeholder=\"###\",\n",
        "    description=\"Stop-Token:\"\n",
        ")\n",
        "\n",
        "html_warning = widgets.HTML(description = \"Status:\", value = \"OK\")\n",
        "\n",
        "# Bisschen breiter anzeigen\n",
        "textbox_max_tokens.layout.width = '200px'\n",
        "dropdown_model.layout.width = '300px'\n",
        "area_system.layout.width = '600px'\n",
        "\n",
        "# Vorbereitungen für die Einstellungen sind getan - jetzt die OpenAI-Libraries\n",
        "print(\"Widgets eingerichtet.\")\n",
        "\n",
        "# Tokenizer Tiktoken einbinden\n",
        "!pip install -q tiktoken\n",
        "import tiktoken\n",
        "print(\"Tokenizer tiktoken geladen.\")\n",
        "\n",
        "# Hilfsfunktion: Token berechnen\n",
        "def num_tokens_from_string(string: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    # cl100k_base ist der Tokenizer für Davinci, GPT-3 und GPT-4\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "# OpenAI-API-Library einbinden\n",
        "!pip install -q openai\n",
        "import openai\n",
        "print(\"OpenAI-API-Library geladen.\")\n",
        "\n",
        "# https://github.com/py-pdf/benchmarks\n",
        "!pip install -q pypdfium2\n",
        "import pypdfium2 as pdfium\n",
        "print(\"PDF-Importer pypdfium geladen.\")\n",
        "\n",
        "# install bark as well as pytorch nightly to get blazing fast flash-attention\n",
        "#!pip install -q git+https://github.com/suno-ai/bark.git && \\\n",
        "#  pip uninstall -y torch torchvision torchaudio && \\\n",
        "#  pip install -q --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu118\n",
        "#from bark import SAMPLE_RATE, generate_audio, preload_models\n",
        "#from IPython.display import Audio\n",
        "\n",
        "#preload_models()\n",
        "#print(\"Audio-Generator Bark geladen.\")\n",
        "\n",
        "# Funktion wird bei Veränderung ausgeführt\n",
        "def update_params(change):\n",
        "    global temperature\n",
        "    global max_tokens\n",
        "    global system_prompt\n",
        "    global system_tokens\n",
        "    global model\n",
        "    global stoptokens\n",
        "    global best_of\n",
        "    temperature = slider_temperature.value\n",
        "    model = dropdown_model.value\n",
        "    best_of = slider_bestof.value\n",
        "    # Token-Obergrenze umrechnen\n",
        "    try:\n",
        "        max_tokens = int(textbox_max_tokens.value)\n",
        "        if max_tokens == 0:\n",
        "            max_tokens = None\n",
        "        else: \n",
        "            limit = models_token_info.get(model)['max_tokens']\n",
        "            if max_tokens + system_tokens > limit: \n",
        "                html_warning = \"<strong><em>Token-Obergrenze des Modells überschritten</em></strong>\"\n",
        "            else:\n",
        "                html_warning = \"OK\"\n",
        "    except ValueError:\n",
        "        max_tokens = None\n",
        "    textbox_max_tokens.value = f'{max_tokens}'\n",
        "    system_prompt = area_system.value\n",
        "    system_tokens = num_tokens_from_string(system_prompt)\n",
        "    stoptokens = textbox_stop.value\n",
        "    if (stoptokens == \"\"):\n",
        "      stoptokens = None\n",
        "\n",
        "# 1x aufrufen.\n",
        "update_params(0)\n",
        "\n",
        "# Verbinde die Widgets mit der Funktion zur Verarbeitung der Werte\n",
        "textbox_max_tokens.observe(update_params, 'value')\n",
        "slider_temperature.observe(update_params, 'value')\n",
        "slider_bestof.observe(update_params, 'value')\n",
        "area_system.observe(update_params, 'value')\n",
        "textbox_stop.observe(update_params, 'value')\n",
        "dropdown_model.observe(update_params, 'value')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Einstellungen für das Modell\n",
        "\n",
        "Basiseinstellungen: Temperatur (tendenziell niedriger als sonst, um reproduzierbare Ergebnisse zu bekommen), Modell, Stop-Token (der Text des PDF wird in Stop-Token eingefasst, um dem Modell eine Sinneinheit zu signalisieren), und das System-Prompt (die Aufgabenbeschreibung). "
      ],
      "metadata": {
        "id": "jAzbYAnDrR37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "from getpass import getpass\n",
        "key_needed = True\n",
        "while key_needed:\n",
        "    openai.api_key = getpass(\"OpenAI-API-Key eingeben: \")\n",
        "    try: \n",
        "        # Testweise Modelle abfragen\n",
        "        models = openai.Model.list()['data']\n",
        "        # Erfolg?\n",
        "        print(\"API-Key gültig!\")\n",
        "        key_needed = False\n",
        "    except: \n",
        "        print(\"Fehler bei Abfrage; ist der API-Key möglicherweise ungültig?\")\n",
        "previous_messages = []\n",
        "previous_tokens = 0\n",
        "spent_tokens = 0\n",
        "spent_dollars = 0.00\n",
        "pdf_tokens = 0\n",
        "area_system.value = \"Du beantwortetst Fragen zum Text. Du erklärst, was im Text \\\n",
        "zu finden ist und was in den Trainingsdaten des Sprachmodells zu finden ist.\"\n",
        "system_prompt = area_system.value\n",
        "system_tokens = num_tokens_from_string(system_prompt)\n",
        "\n",
        "def on_chatbot_reset_clicked(button):\n",
        "    global previous_messages\n",
        "    global spent_tokens\n",
        "    global spent_dollars\n",
        "    global previous_tokens\n",
        "    previous_tokens = 0 \n",
        "    previous_messages = []\n",
        "    spent_tokens = 0\n",
        "    spent_dollars = 0.00\n",
        "    chatbot_output_area.value = ''\n",
        "\n",
        "# Die Einstellungs-Widgets anzeigen\n",
        "# Setzt die globalen Variablen temperature, system_prompt, api_key, model, stoptokens\n",
        "display(slider_temperature,\n",
        "#        slider_bestof,\n",
        "        dropdown_model,\n",
        "        textbox_stop, \n",
        "        textbox_max_tokens,\n",
        "        area_system)\n",
        "\n",
        "# Hilfsfunktion : Kosten berechnen. \n",
        "def pricing(tokens):\n",
        "    price = models_token_info.get(model)['pricing']\n",
        "    # Kosten in Dollar zurückgeben\n",
        "    return(tokens * price / 1000)\n",
        "\n",
        "def chatbot(prompts):\n",
        "    # Prompt \n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=prompts,\n",
        "        n=1,\n",
        "#        best_of = best_of,\n",
        "        stop=stoptokens,\n",
        "        max_tokens=max_tokens,\n",
        "        temperature=temperature,\n",
        "        stream = True\n",
        "    )\n",
        "    return response\n",
        "\n",
        "text_tokens = widgets.HTML(\n",
        "    value = '<b>Verbrauchte Token</b>: 0 ($0.00)'\n",
        ")\n",
        "\n",
        "# Define the widget for displaying token usage\n",
        "def update_token_usage_widget(value):\n",
        "    global spent_tokens\n",
        "    global spent_dollars\n",
        "    spent_tokens += value\n",
        "    spent_dollars += pricing(value)\n",
        "    token_usage_text = f'PDF: {pdf_tokens} Token ($ {pricing(pdf_tokens):.3f}) <br>\\\n",
        "<b>Verbrauchte Token:</b> Diesmal {value} ($ {pricing(value):.3f}), \\\n",
        "insgesamt {spent_tokens} ($ {spent_dollars:.3f}) '\n",
        "    text_tokens.value = token_usage_text\n",
        "\n",
        "chatbot_output_area = widgets.HTML(\n",
        "    value='',\n",
        "    description='Dialog:',\n",
        "    layout=widgets.Layout(width='100%')\n",
        ")\n",
        "# Hilfsfunktion: Token berechnen\n",
        "def calculate_tokens(string: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    # cl100k_base ist der Tokenizer für Davinci, GPT-3 und GPT-4\n",
        "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "# Ausgaben von GPT formatieren:\n",
        "# - \\n in <br> umsetzen\n",
        "# - Codeblöcke mit <pre><code> beginnen und abschließen\n",
        "\n",
        "import re\n",
        "\n",
        "def gptparse(text):\n",
        "    # Codeblöcke\n",
        "    pattern =  r'\\`\\`\\`(?P<text>[^*]+)\\`\\`\\`'\n",
        "    text = re.sub(pattern, r'<code><pre>\\g<text></pre></code>', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "chatbot_output_area = widgets.HTML(\n",
        "    value='',\n",
        "    description='Dialog:',\n",
        "    layout=widgets.Layout(width='100%')\n",
        ")\n",
        "\n",
        "# Define the function to be called when the chatbot is used\n",
        "def on_chatbot_button_clicked(button):\n",
        "    global chatbot_output\n",
        "    global previous_tokens\n",
        "    # Get the user's input and display it\n",
        "    user_input = user_text.value\n",
        "    user_text.value = ''\n",
        "    # Generate a response from the chatbot\n",
        "    chatbot_output_area.value += f'<p style=\"font-family: Verdana;\"><b>Du</b>: {user_input}</p>'\n",
        "    messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": pdf_prompt},\n",
        "            *previous_messages,\n",
        "            {\"role\": \"user\", \"content\": user_input},\n",
        "        ]\n",
        "    chatbot_output_area.value += '<p style=\"font-family: Verdana; font-style: italic;\"><b>Chatbot: </b>' \n",
        "    # Anzahl von Tokens mit der User-Frage initiieren\n",
        "    user_tokens = calculate_tokens(user_input)\n",
        "    # Kosten für die Anfrage einpreisen\n",
        "    update_token_usage_widget(user_tokens + system_tokens + pdf_tokens + previous_tokens)\n",
        "    # Stream-Objekt mit der Antwort\n",
        "    chatbot_response = chatbot(messages)\n",
        "    collected_messages = []   # braucht man nicht zwingend\n",
        "    # Iteriere über die Chunks (die Brocken )\n",
        "    for chunk in chatbot_response:\n",
        "        chunk_message = chunk['choices'][0]['delta']  # extract the message\n",
        "        collected_messages.append(chunk_message)  # save the event response\n",
        "        # Ausgabefenster: Neuen Chunk anhängen\n",
        "        chatbot_output = chunk_message.get('content', '')\n",
        "        chatbot_output_area.value += re.sub('\\r?\\n','<br>',chatbot_output)\n",
        "        update_token_usage_widget(calculate_tokens(chatbot_output))\n",
        "    # Stream-HTML-Block abschließen\n",
        "    chatbot_output_area.value += '</p>'\n",
        "    chatbot_output_area.value = gptparse(chatbot_output_area.value)\n",
        "    # Antwort komplett in die Chathistorie aufnehmen\n",
        "    chatbot_output = ''.join([m.get('content', '') for m in collected_messages])\n",
        "#    audio_array = generate_audio(chatbot_output)\n",
        "#    Audio(audio_array, rate=SAMPLE_RATE)\n",
        "    previous_messages.extend([\n",
        "        {\"role\": \"user\", \"content\": user_input},\n",
        "        {\"role\": \"assistant\", \"content\": chatbot_output},\n",
        "    ])\n",
        "    # Vorigen Text in die Token-Nutzungs-Berechnung einfließen lassen\n",
        "    previous_tokens += calculate_tokens(chatbot_output) + user_tokens\n",
        "\n",
        "# Define the chatbot input and output widgets\n",
        "user_text = widgets.Text(\n",
        "    placeholder='...',\n",
        "    description='Du:',\n",
        "    layout=widgets.Layout(width='60%'),\n",
        ")\n",
        "\n",
        "# Definiere den Absenden-Button und binde ihn an on_chatbot_button_clicked\n",
        "chatbot_button = widgets.Button(\n",
        "    description='Absenden',\n",
        "    layout=widgets.Layout(width='15%'),\n",
        ")\n",
        "chatbot_reset = widgets.Button(\n",
        "    description = 'Reset',\n",
        "    layout=widgets.Layout(width='15%')\n",
        ")\n",
        "chatbot_button.on_click(on_chatbot_button_clicked)\n",
        "chatbot_reset.on_click(on_chatbot_reset_clicked)\n",
        "# Abschicken auch durch Return in der user_text Box\n",
        "user_text.on_submit(on_chatbot_button_clicked)\n",
        "\n"
      ],
      "metadata": {
        "id": "4z2rV5r0nTna",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF einlesen, an den Chatbot übergeben, Eingaben erwarten\n",
        "\n",
        "Nach der Eingabe des OpenAI-API-Tokens lädt das Skript ein PDF hoch, extrahiert den Text, und ermöglicht eine Konversation darüber. "
      ],
      "metadata": {
        "id": "vMjK8OSw6-vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "condense_checkbox = widgets.ToggleButton(\n",
        "    value=False,\n",
        "    description='Aktiviere Condense Mode',\n",
        "    layout=widgets.Layout(width='300px')\n",
        ")\n",
        "\n",
        "def activate_condense_mode(change):\n",
        "    global system_prompt\n",
        "    global pdf_prompt\n",
        "    global pdf_tokens\n",
        "    global file_upload\n",
        "    global text_all \n",
        "    condense_checkbox.value = True\n",
        "    condense_checkbox.disabled = True\n",
        "    # Chat-Widgets verschwinden lassen\n",
        "    user_text.disabled = True\n",
        "    pdf_name = str(list(file_upload.keys())[0])\n",
        "    pdf = pdfium.PdfDocument(file_upload[pdf_name])\n",
        "    n_pages = len(pdf)\n",
        "    text_all = \"\"\n",
        "    seite_html = widgets.HTML()\n",
        "    fortschritt = widgets.IntProgress(\n",
        "        value=0,\n",
        "        min=0,\n",
        "        max=n_pages,\n",
        "        description='Komprimiere: ',\n",
        "        orientation='horizontal'\n",
        "    )\n",
        "    condense_checkbox.close()\n",
        "    display(seite_html)\n",
        "    display(fortschritt)\n",
        "    display(text_tokens)\n",
        "    # Setze das Modell vorübergehend auf GPT3.5-Turbo (aus Kostengründen)\n",
        "    model = 'gpt-3.5-turbo'\n",
        "    for page_num,page in enumerate(pdf):\n",
        "        text_all += f\"\\n\\nSEITE: {page_num+1}\\n\\nZUSAMMENFASSUNG: \"\n",
        "        # Load a text page helper\n",
        "        textpage = page.get_textpage()\n",
        "        # Extract text from the whole page and condense\n",
        "        messages = [\n",
        "              {\"role\": \"system\", \"content\": \"Erstelle eine knappe Zusammenfassung des Textes.\"},\n",
        "              {\"role\": \"user\", \"content\": textpage.get_text_range()}\n",
        "          ]\n",
        "        seite_html.value = f\"Seite: {page_num+1} Zeichen: {len(textpage.get_text_range())} Token ca: {num_tokens_from_string(textpage.get_text_range())}\"\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=model,\n",
        "            messages=messages,\n",
        "            n=1,\n",
        "            temperature=0.0,\n",
        "            stream = False\n",
        "        ) \n",
        "        text_all += response['choices'][0]['message']['content'] + '\\n###\\n'\n",
        "        # Update Kosten\n",
        "        update_token_usage_widget(response[\"usage\"][\"prompt_tokens\"])\n",
        "        # Update Fortschrittsbalken\n",
        "        fortschritt.value = page_num+1  \n",
        "    # Fertig\n",
        "    fortschritt.close()\n",
        "    system_prompt = \"Beantworte Fragen zu einem Text. Inhaltsangaben für jede Seite \\\n",
        "folgen. Wenn du Antworten zum Text auf einer Seite N geben sollst, antworte mit \\\n",
        "'GETTEXT N', um den gesamten Text lesen zu können.\"\n",
        "    area_system.value = system_prompt\n",
        "    # PDF-Volltext durch Kondensat ersetzen\n",
        "    pdf_prompt = text_all\n",
        "    pdf_tokens = num_tokens_from_string(text_all)\n",
        "    # Token-Verbrauchsanzeige auf korrekte Werte für das PDF und 0 fürs aktuelle Prompt\n",
        "    update_token_usage_widget(0)\n",
        "    model = dropdown_model.value\n",
        "    user_text.disabled = False\n",
        "    display(chatbot_output_area, user_text, chatbot_button, chatbot_reset)\n",
        "\n",
        "# Activate ausführen, sobald Checkbox geklickt wird\n",
        "condense_checkbox.observe(activate_condense_mode, names='value')\n",
        "\n",
        "def import_pdf(file_upload):\n",
        "  global pdf_prompt\n",
        "  # Namen des hochgeladenen PDF extrahieren\n",
        "  pdf_name = str(list(file_upload.keys())[0])\n",
        "  pdf = pdfium.PdfDocument(file_upload[pdf_name])\n",
        "  # n_pages = len(pdf)\n",
        "  # Das ist erst mal ganz stumpf: Iteriere durch alle Seiten und hänge sie \n",
        "  # an einen großen Python-String. \n",
        "  #\n",
        "  # Verfeinerte Strukturierung - Seitenzahlen oä - später. \n",
        "  text_all = \"\"\n",
        "  for page_num,page in enumerate(pdf):\n",
        "      text_all += f\"\\n\\nSEITE: {page_num+1}\\n\\n\"\n",
        "      # Load a text page helper\n",
        "      textpage = page.get_textpage()\n",
        "      # Extract text from the whole page\n",
        "      text_all += textpage.get_text_range()\n",
        "  # Update Kosten\n",
        "  return(text_all)\n",
        "\n",
        "file_upload = files.upload()\n",
        "text_all = import_pdf(file_upload)\n",
        "pdf_tokens = num_tokens_from_string(text_all)\n",
        "print(f\"{len(text_all)} Zeichen - in Token: {pdf_tokens}\")\n",
        "# PDF länger als Token-Fenster plus Reserven?\n",
        "if pdf_tokens + system_tokens + 200 > models_token_info.get(model)['max_tokens']:\n",
        "    print(\"Token-Fenster zu klein; Condense-Modus erforderlich.\")\n",
        "    print(\"Dieser Modus erstellt Zusammenfassungen aller Seiten und arbeitet damit.\")\n",
        "    print(\"Ein Klick auf den Button startet die Zusammenfassungen (mit gpt-3.5)\")\n",
        "    # Legt gleichzeitig die Checkbox lahm. \n",
        "    # Chat-Widgets verschwinden lassen\n",
        "    force_condense_mode = True\n",
        "else: \n",
        "    force_condense_mode = False\n",
        "display(condense_checkbox)\n",
        "\n",
        "\n",
        "pdf_prompt = \"###\\nAnalysiere diesen Text:\\n\" + text_all + \"\\n###\\n\"\n",
        "# audio_array = generate_audio(system_prompt)\n",
        "# Audio(audio_array, rate=SAMPLE_RATE)\n",
        "\n",
        "\n",
        "# Die eigentliche Chatbot-Funktion findet sich in der Funktion\n",
        "# on_chatbot_button_clicked()\n",
        "\n",
        "# Display the chatbot widgets - wenn Condense Mode nicht nötig\n",
        "if (not force_condense_mode):\n",
        "    display(html_warning)\n",
        "    display(text_tokens)\n",
        "    display(chatbot_output_area, user_text, chatbot_button, chatbot_reset)\n"
      ],
      "metadata": {
        "id": "_quhuhEKrSi8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bekannte Probleme\n",
        "\n",
        "- Manchmal verschluckt das Eingabefeld die letzten 1, 2 Zeichen\n",
        "- Reset-Button funktioniert nicht (sollte Neuladen triggern)\n",
        "- Obergrenze des Modells werden derzeit nicht angezeigt\n",
        "- Token-Berechnung beruht auf dem Standardmodell und ist deshalb ungenau\n",
        "\n",
        "## Nice-to-have: Verbesserungen\n",
        "\n",
        "- Zeichen-/Token-Angabe je Seite beim Condense (Hilft auch, leere Dokumente zu identifizieren) \n",
        "- Inhaltsverzeichnis des PDF generieren und ebenfalls übergeben\n",
        "- Den \"Zusammenfassungsmodus\" um die Möglichkeit ergänzen, dem Modell den Volltext einer Seite zur Verfügung zu stellen - über ```GETPAGE n``` \n",
        "- Codeblöcke korrekt formatieren\n",
        "- Liste der Modelle auf die beschränken, die für das API-Token freigegeben sind\n",
        "- Modellgenaue Token-Abrechnung\n",
        "\n",
        "## Versionshistorie\n",
        "\n",
        "- **V1.1beta**: Token-Verbrauchsangabe für die Zusammenfassung korrigiert; jetzt kann man die Kosten des PDF-Kondensats einigermaßen abschätzen. \n",
        "- **V1.1alpha**: Experimenteller \"Condense-Modus\" erstellt Zusammenfassungen für jede Seite und arbeitet mit diesen.\n",
        "- **V1.0**: Funktionsfähig für Dokumente unterhalb der Token-Grenze des Modells; Streaming (stückweise Rückgabe der Ergebnisse wie bei ChatGPT) ermöglicht\n",
        "- **V0.1**: Erste Funktionsdemo für PDF-Extraktion und Chat-Completion"
      ],
      "metadata": {
        "id": "YIoVYGxYmQcz"
      }
    }
  ]
}